{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JSC270_A4Q1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sean-1005/JSC270_A4/blob/main/JSC270_A4Q1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvnUqyaZfMIN"
      },
      "source": [
        "**Question 1** Sentiment Analysis with a Common Twitter Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA52DvX0pX4Q"
      },
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "xsHBJx2o1nGX",
        "outputId": "e5ce52db-dc42-4003-f6e5-a2db0910acd3"
      },
      "source": [
        "# upload the training dataset (covid-tweets-train.csv)\n",
        "df_train = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e583749a-06b8-4314-a7cd-48568fb46200\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e583749a-06b8-4314-a7cd-48568fb46200\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving covid-tweets-train.csv to covid-tweets-train (3).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "ROUs5DQv2GeP",
        "outputId": "9c98426d-1285-4391-dcaf-b8c75bc9a4e0"
      },
      "source": [
        "# upload the testing dataset (covid-tweets-test.csv)\n",
        "df_test = files.upload()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-88303574-420d-4433-8342-459ff1e337cf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-88303574-420d-4433-8342-459ff1e337cf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving covid-tweets-test.csv to covid-tweets-test (2).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwriiRJx23v0"
      },
      "source": [
        "# Read the csv files\n",
        "df_train = pd.read_csv(io.BytesIO(df_train['covid-tweets-train.csv']))\n",
        "df_test = pd.read_csv(io.BytesIO(df_test['covid-tweets-test.csv']))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uMdqyT3Jwxz",
        "outputId": "c3d8e0f3-bed2-4db3-ddba-b3e003037c99"
      },
      "source": [
        "# Check invalid values\n",
        "df_train.info()\n",
        "df_train['Sentiment'].value_counts()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 41155 entries, 0 to 41154\n",
            "Data columns (total 3 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   Unnamed: 0     41155 non-null  int64 \n",
            " 1   OriginalTweet  41155 non-null  object\n",
            " 2   Sentiment      41153 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 964.7+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2            18042\n",
              "0            15397\n",
              "1             7712\n",
              " PA\"             1\n",
              " England\"        1\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw05cDLnptuH"
      },
      "source": [
        "There is 2 Nan and 2 invalid values in the df_train['Sentiment']."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6H5B3GkKDEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "960ef12f-9af8-4407-a9ef-7ebdf49297ad"
      },
      "source": [
        "#cleaning data\n",
        "# Find the row number of the four invalid values for Nan and Sentiment PA\", and England\". \n",
        "index = []\n",
        "for i in range(len(df_train['Sentiment'])):\n",
        "  if df_train['Sentiment'][i] != '0' and df_train['Sentiment'][i] != '1' and df_train['Sentiment'][i] != '2':\n",
        "    index.append(i)\n",
        "\n",
        "print(index)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[15269, 18450, 21383, 36128]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEkSVXe6mp6T",
        "outputId": "e0f30b58-81ec-4959-d8f3-5ec35ea90fbf"
      },
      "source": [
        "# print the sentiment with corresponding row number\n",
        "print(df_train['Sentiment'][15269])\n",
        "print(df_train['Sentiment'][18450])\n",
        "print(df_train['Sentiment'][21383])\n",
        "print(df_train['Sentiment'][36128])\n",
        "\n",
        "# print the tweet with corresponding row number\n",
        "print(\"The 21383 tweet:\", df_train['OriginalTweet'][18450])\n",
        "print(\"The 36128 tweet\", df_train['OriginalTweet'][36128])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nan\n",
            " PA\"\n",
            "nan\n",
            " England\"\n",
            "The 21383 tweet: @realDonaldTrump @POTUS @RandPaul Donald J. Trump, IÂm worried about the economic fallout of this covid-19 Âshelter in placeÂ, in combination with crashing oil prices!!! IÂll cut to the chase. During the holidays my credit union has a program called ?,Positive\r\r\n",
            "22250,67202,Cranberry Township\n",
            "The 36128 tweet @TheBlinkingOwl joined the growing list of distilleries producing and/or donating hand sanitizer to ease the shortage! WeÂre thankful for their generous donations of 5-gallon drums of sanitizer, three to @SouthCoastGMC and two to @ChapmanGMC.\r\r\n",
            "\r\r\n",
            "#COVID19?,Extremely Positive\r\r\n",
            "39929,84881,Worthing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nLMz5nCfddV"
      },
      "source": [
        "# Label the tweets based on theor content\n",
        "df_train.loc[18450, 'Sentiment'] = '0'\n",
        "df_train.loc[36128, 'Sentiment'] = '2'\n",
        "# drop the Nan rows\n",
        "df_train.dropna(subset = ['Sentiment'], inplace = True)\n",
        "df_train['Sentiment']  = df_train['Sentiment'].astype(int)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICZ0qkRCf9fB"
      },
      "source": [
        "(A) Consider the training data. What is the balance between the three classes? In other words, what\n",
        "proportion of the observations (in the training set) belong to each class?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kcp-Ym5AgIig"
      },
      "source": [
        "df_train.Sentiment.value_counts()\n",
        "df_train.info()\n",
        "df_train[\"Sentiment\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPgHxCrzjsWt"
      },
      "source": [
        "The proportion of 'positive' is $\\frac{18043}{41153}=43.84\\%$. \n",
        "\n",
        "The proportion of 'neutral' is $\\frac{7712}{41153}=18.74\\%$.\n",
        "\n",
        "The proportion of 'negative' is $\\frac{15398}{41153}=37.42\\%$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAImbrAshqZL"
      },
      "source": [
        "(B) Tokenize the tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rSnLUvM45Mi"
      },
      "source": [
        "# Create a new column in df_train and df_test that contains token lists\n",
        "def tokenize(df):\n",
        "  token = []\n",
        "  for tweet in df['OriginalTweet']:\n",
        "    token.append(tweet.split())\n",
        "  return token\n",
        "\n",
        "df_train['tokens'] = tokenize(df_train)\n",
        "df_test['tokens'] = tokenize(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1_4DkZspZ8l"
      },
      "source": [
        "(C) Remove URL tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7lHZ9QypfHB"
      },
      "source": [
        "import re\n",
        "\n",
        "def no_links(df):\n",
        "  tokens_no_links = []\n",
        "  for tweet in df['tokens']:\n",
        "    tokens_no_links.append([re.sub('^http.*$','', t) for t in tweet])\n",
        "  return tokens_no_links\n",
        "\n",
        "df_train['tokens'] = no_links(df_train)\n",
        "df_test['tokens'] = no_links(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzJ64ZkNtTQ7"
      },
      "source": [
        "(D) Remove all punctuation and convert all the tokens to lowercase\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc8cVnShtbFJ"
      },
      "source": [
        "# Remove all punctuations\n",
        "temp_train = []\n",
        "temp_test = []\n",
        "tokens_no_punct_train = []\n",
        "tokens_no_punct_test = []\n",
        "\n",
        "for row in df_train[\"tokens\"]:\n",
        "  temp_train.append([re.sub('[^\\w\\s]','', t) for t in row])\n",
        "\n",
        "for row in temp_train:\n",
        "  tokens_no_punct_train.append([w for w in row if w != ''])\n",
        "\n",
        "for row in df_test[\"tokens\"]:\n",
        "  temp_test.append([re.sub('[^\\w\\s]','', t) for t in row])\n",
        "\n",
        "for row in temp_test:\n",
        "  tokens_no_punct_test.append([w for w in row if w != ''])\n",
        "\n",
        "# Covert all row into lowercase\n",
        "tokens_lowercase_train = []\n",
        "tokens_lowercase_test = []\n",
        "for row in tokens_no_punct_train:\n",
        "  tokens_lowercase_train.append([t.lower() for t in row])\n",
        "\n",
        "for row in tokens_no_punct_test:\n",
        "  tokens_lowercase_test.append([t.lower() for t in row])\n",
        "\n",
        "df_train[\"tokens\"] = tokens_lowercase_train\n",
        "df_test[\"tokens\"] = tokens_lowercase_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEz9iCqn3EkV"
      },
      "source": [
        "（E）Stemming using PorterStemmer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqkfj9g_3F_K"
      },
      "source": [
        "from nltk.stem.porter import *\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stem(df):\n",
        "  stemed = []\n",
        "  for tweet in df['tokens']:\n",
        "    stemed.append([stemmer.stem(w) for w in tweet])\n",
        "  return stemed\n",
        "\n",
        "df_train[\"tokens_stem\"] = stem(df_train)\n",
        "df_test[\"tokens_stem\"] = stem(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBlP0qCD2oro"
      },
      "source": [
        "(F) Remove teh first 100 stopwords in the lists of stopwords fromnltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZwT-qSX_YrJ"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "sw = stopwords.words('english')[:100]\n",
        "\n",
        "tokens_no_sw_train = []\n",
        "tokens_no_sw_test = []\n",
        "\n",
        "for row in df_train['tokens_stem']:\n",
        "  tokens_no_sw_train.append([w for w in row if w not in sw])\n",
        "\n",
        "df_train['tokens_stem'] = tokens_no_sw_train\n",
        "\n",
        "for row in df_test['tokens_stem']:\n",
        "  tokens_no_sw_test.append([w for w in row if w not in sw])\n",
        "\n",
        "df_test['tokens_stem'] = tokens_no_sw_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGzCv2s7AfSS"
      },
      "source": [
        "(G) Convert lists of words into vectors of word counts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qhp1m29VVp1G"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Separate labels from features, converting to numpy arrays\n",
        "X_train, y_train = df_train['tokens_stem'].to_numpy(), df_train['Sentiment'].to_numpy()\n",
        "X_test, y_test = df_test['tokens_stem'].to_numpy(), df_test['Sentiment'].to_numpy()\n",
        "\n",
        "def override_fcn(doc):\n",
        "  # We expect a list of tokens as input\n",
        "  return doc\n",
        "\n",
        "vocab = (np.unique(np.concatenate(X_train))).shape[0]\n",
        "\n",
        "# Count Vectorizer\n",
        "count_vec = CountVectorizer(\n",
        "    analyzer='word',\n",
        "    tokenizer= override_fcn,\n",
        "    preprocessor= override_fcn,\n",
        "    token_pattern= None,\n",
        "    max_features = 1000)\n",
        "\n",
        "# Remember this output is a Scipy Sparse Array\n",
        "X = np.concatenate((X_train, X_test), axis=0)\n",
        "counts = count_vec.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU3ck3OzZrkI"
      },
      "source": [
        "print(len(count_vec.vocabulary_)) # length of the vocabulary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFlVl3RjKay0"
      },
      "source": [
        "(H) Fit Naive Bayes model using count vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9ilTECvBPWA"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "X_train, X_test = counts[:np.size(X_train), : ], counts[np.size(X_train):, :]\n",
        "\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "predict = nb.predict(X_test)\n",
        "\n",
        "print('Test accuracy with simple Naive Bayes:',accuracy_score(y_test, predict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fUOrcHQ857G"
      },
      "source": [
        "# FInd the top 5 words for each class\n",
        "positive = []\n",
        "neutral = []\n",
        "negative = []\n",
        "\n",
        "X, y = df_train['tokens_stem'].to_numpy(), df_train['Sentiment'].to_numpy()\n",
        "\n",
        "for i in range(len(X)):\n",
        "  if y[i] == 0:\n",
        "    negative.append(X[i])\n",
        "  elif y[i] == 1:\n",
        "    neutral.append(X[i])\n",
        "  else:\n",
        "    positive.append(X[i])\n",
        "\n",
        "count_vec_postop = CountVectorizer(\n",
        "    analyzer='word',\n",
        "    tokenizer= override_fcn,\n",
        "    preprocessor= override_fcn,\n",
        "    token_pattern= None,\n",
        "    max_features = 1000)\n",
        "\n",
        "# Remember this output is a Scipy Sparse Array\n",
        "count_vec_postop.fit_transform(positive)\n",
        "\n",
        "# Print the names of each of the features (1000 total))\n",
        "token_freq_p = count_vec_postop.vocabulary_\n",
        "\n",
        "count_vec_neutop = CountVectorizer(\n",
        "    analyzer='word',\n",
        "    tokenizer= override_fcn,\n",
        "    preprocessor= override_fcn,\n",
        "    token_pattern= None,\n",
        "    max_features = 1000)\n",
        "\n",
        "# Remember this output is a Scipy Sparse Array\n",
        "count_vec_neutop.fit_transform(neutral)\n",
        "\n",
        "# Print the names of each of the features (1000 total))\n",
        "token_freq_neu = count_vec_neutop.vocabulary_\n",
        "\n",
        "count_vec_negtop = CountVectorizer(\n",
        "    analyzer='word',\n",
        "    tokenizer= override_fcn,\n",
        "    preprocessor= override_fcn,\n",
        "    token_pattern= None,\n",
        "    max_features = 1000)\n",
        "\n",
        "# Remember this output is a Scipy Sparse Array\n",
        "count_vec_negtop.fit_transform(negative)\n",
        "\n",
        "# Print the names of each of the features (1000 total))\n",
        "token_freq_n = count_vec_negtop.vocabulary_\n",
        "\n",
        "\n",
        "sort_orders_p = sorted(token_freq_p.items(), key=lambda x: x[1], reverse=True)\n",
        "sort_orders_neu = sorted(token_freq_neu.items(), key=lambda x: x[1], reverse=True)\n",
        "sort_orders_n = sorted(token_freq_n.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"Top 5 words for 2: \", sort_orders_p[:5])\n",
        "print(\"Top 5 words for 1: \", sort_orders_neu[:5])\n",
        "print(\"Top 5 words for 0: \", sort_orders_n[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs-o1qmInDpM"
      },
      "source": [
        "(I) ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJsGakGfnLqw"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "fpr, tpr, thresholds = roc_curve(y_test, predict, pos_label = 1)\n",
        "\n",
        "plt.plot(fpr,tpr)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operator Characteristic (ROC) Curve for our model')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISFpIyq_m3BS"
      },
      "source": [
        "J) Fit the Naive Bayes model using TF-IDF vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5bozxfL8ckl"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Separate labels from features, converting to numpy arrays\n",
        "X_train, y_train = df_train['tokens_stem'].to_numpy(), df_train['Sentiment'].to_numpy()\n",
        "X_test, y_test = df_test['tokens_stem'].to_numpy(), df_test['Sentiment'].to_numpy()\n",
        "\n",
        "def override_fcn(doc):\n",
        "  # We expect a list of tokens as input\n",
        "  return doc\n",
        "\n",
        "vocab = (np.unique(np.concatenate(X_train))).shape[0]\n",
        "\n",
        "# Count Vectorizer\n",
        "count_vec = CountVectorizer(\n",
        "    analyzer='word',\n",
        "    tokenizer= override_fcn,\n",
        "    preprocessor= override_fcn,\n",
        "    token_pattern= None,\n",
        "    max_features = 1000)\n",
        "\n",
        "# Remember this output is a Scipy Sparse Array\n",
        "X = np.concatenate((X_train, X_test), axis=0)\n",
        "counts = count_vec.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsLg1xoe7t7N"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "######for training data\n",
        "tfidf = TfidfTransformer()\n",
        "tfs1 = tfidf.fit_transform(counts)\n",
        "\n",
        "# Let's use the TFIDF counts for modelling\n",
        "X = tfs1.toarray()\n",
        "X_train, X_test = X[:np.size(X_train), : ], X[np.size(X_train):, :]\n",
        "\n",
        "# Let's fit the Naive Bayes model to our training data\n",
        "nb = MultinomialNB()\n",
        "# Fit model to training data\n",
        "nb.fit(X_train, y_train)\n",
        "# Predict on test data\n",
        "y_preds = nb.predict(X_test)\n",
        "X_test, y_test = df_test['tokens_stem'].to_numpy(), df_test['Sentiment'].to_numpy()\n",
        "print('Test accuracy with simple Naive Bayes:',accuracy_score(y_test,y_preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvW8xM9WsYRQ"
      },
      "source": [
        "(K) Fit teh Naive Bayes model using lemmatization instead of Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zFpHoVXsTgv"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lem_tokens = []\n",
        "for row in df_train['tokens']:\n",
        "  lem_tokens.append([lemmatizer.lemmatize(t) for t in row])\n",
        "\n",
        "df_train['tokens_lem'] = lem_tokens\n",
        "\n",
        "lem_tokens2 = []\n",
        "for row in df_test['tokens']:\n",
        "  lem_tokens2.append([lemmatizer.lemmatize(t) for t in row])\n",
        "\n",
        "df_test['tokens_lem'] = lem_tokens2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36g1XVUvw6bw"
      },
      "source": [
        "sw_lem = stopwords.words('english')[:100]\n",
        "\n",
        "tokens_no_sw_train_lem = []\n",
        "tokens_no_sw_test_lem = []\n",
        "\n",
        "for row in df_train['tokens_lem']:\n",
        "  tokens_no_sw_train_lem.append([w for w in row if w not in sw_lem])\n",
        "\n",
        "df_train['tokens_lem'] = tokens_no_sw_train_lem\n",
        "\n",
        "for row in df_test['tokens_lem']:\n",
        "  tokens_no_sw_test_lem.append([w for w in row if w not in sw_lem])\n",
        "\n",
        "df_test['tokens_lem'] = tokens_no_sw_test_lem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn637czGtaTj"
      },
      "source": [
        "X_train, y_train = df_train['tokens_lem'].to_numpy(), df_train['Sentiment'].to_numpy()\n",
        "X_test, y_test = df_test['tokens_lem'].to_numpy(), df_test['Sentiment'].to_numpy()\n",
        "\n",
        "def override_fcn(doc):\n",
        "  # We expect a list of tokens as input\n",
        "  return doc\n",
        "\n",
        "# Count Vectorizer\n",
        "count_vec = CountVectorizer(\n",
        "    analyzer='word',\n",
        "    tokenizer= override_fcn,\n",
        "    preprocessor= override_fcn,\n",
        "    token_pattern= None,\n",
        "    max_features = 1000)\n",
        "\n",
        "# Remember this output is a Scipy Sparse Array\n",
        "X = np.concatenate((X_train, X_test), axis=0)\n",
        "counts = count_vec.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTWHoGesurzW"
      },
      "source": [
        "tfidf = TfidfTransformer()\n",
        "tfs1 = tfidf.fit_transform(counts)\n",
        "\n",
        "# Let's use the TFIDF counts for modelling\n",
        "X = tfs1.toarray()\n",
        "X_train, X_test = X[:np.size(X_train), : ], X[np.size(X_train):, :]\n",
        "\n",
        "# Let's fit the Naive Bayes model to our training data\n",
        "nb = MultinomialNB()\n",
        "# Fit model to training data\n",
        "\n",
        "nb.fit(X_train, y_train)\n",
        "# Predict on test data\n",
        "y_preds = nb.predict(X_test)\n",
        "X_test, y_test = df_test['tokens_lem'].to_numpy(), df_test['Sentiment'].to_numpy()\n",
        "print('Test accuracy with simple Naive Bayes:',accuracy_score(y_test,y_preds))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}